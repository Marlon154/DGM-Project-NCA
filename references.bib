@online{choi2022perception,
  title = {Perception {{Prioritized Training}} of {{Diffusion Models}}},
  author = {Choi, Jooyoung and Lee, Jungbeom and Shin, Chaehun and Kim, Sungwon and Kim, Hyunwoo and Yoon, Sungroh},
  date = {2022-04-01},
  eprint = {2204.00227},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2204.00227},
  urldate = {2024-04-25},
  abstract = {Diffusion models learn to restore noisy data, which is corrupted with different levels of noise, by optimizing the weighted sum of the corresponding loss terms, i.e., denoising score matching loss. In this paper, we show that restoring data corrupted with certain noise levels offers a proper pretext task for the model to learn rich visual concepts. We propose to prioritize such noise levels over other levels during training, by redesigning the weighting scheme of the objective function. We show that our simple redesign of the weighting scheme significantly improves the performance of diffusion models regardless of the datasets, architectures, and sampling strategies.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/marlon/Zotero/storage/V6D8RL57/Choi et al. - 2022 - Perception Prioritized Training of Diffusion Model.pdf;/home/marlon/Zotero/storage/KGNKA9UA/2204.html}
}

@online{du2022vos,
  title = {{{VOS}}: {{Learning What You Don}}'t {{Know}} by {{Virtual Outlier Synthesis}}},
  shorttitle = {{{VOS}}},
  author = {Du, Xuefeng and Wang, Zhaoning and Cai, Mu and Li, Yixuan},
  date = {2022-05-09},
  eprint = {2202.01197},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2202.01197},
  urldate = {2024-04-25},
  abstract = {Out-of-distribution (OOD) detection has received much attention lately due to its importance in the safe deployment of neural networks. One of the key challenges is that models lack supervision signals from unknown data, and as a result, can produce overconfident predictions on OOD data. Previous approaches rely on real outlier datasets for model regularization, which can be costly and sometimes infeasible to obtain in practice. In this paper, we present VOS, a novel framework for OOD detection by adaptively synthesizing virtual outliers that can meaningfully regularize the model's decision boundary during training. Specifically, VOS samples virtual outliers from the low-likelihood region of the class-conditional distribution estimated in the feature space. Alongside, we introduce a novel unknown-aware training objective, which contrastively shapes the uncertainty space between the ID data and synthesized outlier data. VOS achieves competitive performance on both object detection and image classification models, reducing the FPR95 by up to 9.36\% compared to the previous best method on object detectors. Code is available at https://github.com/deeplearning-wisc/vos.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/marlon/Zotero/storage/YKLREWSM/Du et al. - 2022 - VOS Learning What You Don't Know by Virtual Outli.pdf;/home/marlon/Zotero/storage/PGFFGTGB/2202.html}
}

@online{du2023generative,
  title = {Generative {{Models}}: {{What}} Do They Know? {{Do}} They Know Things? {{Let}}'s Find Out!},
  shorttitle = {Generative {{Models}}},
  author = {Du, Xiaodan and Kolkin, Nicholas and Shakhnarovich, Greg and Bhattad, Anand},
  date = {2023-11-28},
  eprint = {2311.17137},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2311.17137},
  urldate = {2024-04-25},
  abstract = {Generative models have been shown to be capable of synthesizing highly detailed and realistic images. It is natural to suspect that they implicitly learn to model some image intrinsics such as surface normals, depth, or shadows. In this paper, we present compelling evidence that generative models indeed internally produce high-quality scene intrinsic maps. We introduce Intrinsic LoRA (I LoRA), a universal, plug-and-play approach that transforms any generative model into a scene intrinsic predictor, capable of extracting intrinsic scene maps directly from the original generator network without needing additional decoders or fully fine-tuning the original network. Our method employs a Low-Rank Adaptation (LoRA) of key feature maps, with newly learned parameters that make up less than 0.6\% of the total parameters in the generative model. Optimized with a small set of labeled images, our model-agnostic approach adapts to various generative architectures, including Diffusion models, GANs, and Autoregressive models. We show that the scene intrinsic maps produced by our method compare well with, and in some cases surpass those generated by leading supervised techniques.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning},
  file = {/home/marlon/Zotero/storage/8AHFNI6D/Du et al. - 2023 - Generative Models What do they know Do they know.pdf;/home/marlon/Zotero/storage/LYBCZTWT/2311.html}
}

@incollection{frisch2023synthesising,
  title = {Synthesising {{Rare Cataract Surgery Samples}} with {{Guided Diffusion Models}}},
  booktitle = {Medical {{Image Computing}} and {{Computer Assisted Intervention}} – {{MICCAI}} 2023},
  author = {Frisch, Yannik and Fuchs, Moritz and Sanner, Antoine and Ucar, Felix Anton and Frenzel, Marius and Wasielica-Poslednik, Joana and Gericke, Adrian and Wagner, Felix Mathias and Dratsch, Thomas and Mukhopadhyay, Anirban},
  editor = {Greenspan, Hayit and Madabhushi, Anant and Mousavi, Parvin and Salcudean, Septimiu and Duncan, James and Syeda-Mahmood, Tanveer and Taylor, Russell},
  date = {2023},
  volume = {14228},
  pages = {354--364},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-43996-4_34},
  url = {https://link.springer.com/10.1007/978-3-031-43996-4_34},
  urldate = {2024-04-25},
  isbn = {978-3-031-43995-7 978-3-031-43996-4},
  langid = {english},
  file = {/home/marlon/Zotero/storage/YHHYRECL/Frisch et al. - 2023 - Synthesising Rare Cataract Surgery Samples with Gu.pdf}
}

@online{gargary2024systematic,
  title = {A {{Systematic Review}} of {{Federated Generative Models}}},
  author = {Gargary, Ashkan Vedadi and De Cristofaro, Emiliano},
  date = {2024-05-26},
  eprint = {2405.16682},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2405.16682},
  urldate = {2024-06-12},
  abstract = {Federated Learning (FL) has emerged as a solution for distributed systems that allow clients to train models on their data and only share models instead of local data. Generative Models are designed to learn the distribution of a dataset and generate new data samples that are similar to the original data. Many prior works have tried proposing Federated Generative Models. Using Federated Learning and Generative Models together can be susceptible to attacks, and designing the optimal architecture remains challenging. This survey covers the growing interest in the intersection of FL and Generative Models by comprehensively reviewing research conducted from 2019 to 2024. We systematically compare nearly 100 papers, focusing on their FL and Generative Model methods and privacy considerations. To make this field more accessible to newcomers, we highlight the state-of-the-art advancements and identify unresolved challenges, offering insights for future research in this evolving field.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {/home/marlon/Zotero/storage/W8F5DCMF/Gargary and De Cristofaro - 2024 - A Systematic Review of Federated Generative Models.pdf;/home/marlon/Zotero/storage/DP2ZUS9Y/2405.html}
}

@online{hataya2023will,
  title = {Will {{Large-scale Generative Models Corrupt Future Datasets}}?},
  author = {Hataya, Ryuichiro and Bao, Han and Arai, Hiromi},
  date = {2023-08-09},
  eprint = {2211.08095},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2211.08095},
  urldate = {2024-04-25},
  abstract = {Recently proposed large-scale text-to-image generative models such as DALL\$\textbackslash cdot\$E 2, Midjourney, and StableDiffusion can generate high-quality and realistic images from users' prompts. Not limited to the research community, ordinary Internet users enjoy these generative models, and consequently, a tremendous amount of generated images have been shared on the Internet. Meanwhile, today's success of deep learning in the computer vision field owes a lot to images collected from the Internet. These trends lead us to a research question: "\textbackslash textbf\{will such generated images impact the quality of future datasets and the performance of computer vision models positively or negatively?\}" This paper empirically answers this question by simulating contamination. Namely, we generate ImageNet-scale and COCO-scale datasets using a state-of-the-art generative model and evaluate models trained with "contaminated" datasets on various tasks, including image classification and image generation. Throughout experiments, we conclude that generated images negatively affect downstream performance, while the significance depends on tasks and the amount of generated images. The generated datasets and the codes for experiments will be publicly released for future research. Generated datasets and source codes are available from \textbackslash url\{https://github.com/moskomule/dataset-contamination\}.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/marlon/Zotero/storage/ZZ2Z9B9G/Hataya et al. - 2023 - Will Large-scale Generative Models Corrupt Future .pdf;/home/marlon/Zotero/storage/TTZ4S6KD/2211.html}
}

@online{ho2022video,
  title = {Video {{Diffusion Models}}},
  author = {Ho, Jonathan and Salimans, Tim and Gritsenko, Alexey and Chan, William and Norouzi, Mohammad and Fleet, David J.},
  date = {2022-06-22},
  eprint = {2204.03458},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2204.03458},
  urldate = {2024-04-25},
  abstract = {Generating temporally coherent high fidelity video is an important milestone in generative modeling research. We make progress towards this milestone by proposing a diffusion model for video generation that shows very promising initial results. Our model is a natural extension of the standard image diffusion architecture, and it enables jointly training from image and video data, which we find to reduce the variance of minibatch gradients and speed up optimization. To generate long and higher resolution videos we introduce a new conditional sampling technique for spatial and temporal video extension that performs better than previously proposed methods. We present the first results on a large text-conditioned video generation task, as well as state-of-the-art results on established benchmarks for video prediction and unconditional video generation. Supplementary material is available at https://video-diffusion.github.io/},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/marlon/Zotero/storage/Z5GJFMX3/Ho et al. - 2022 - Video Diffusion Models.pdf;/home/marlon/Zotero/storage/L5BZ7LAE/2204.html}
}

@online{hu2019decentralized,
  title = {Decentralized {{Federated Learning}}: {{A Segmented Gossip Approach}}},
  shorttitle = {Decentralized {{Federated Learning}}},
  author = {Hu, Chenghao and Jiang, Jingyan and Wang, Zhi},
  date = {2019-08-21},
  eprint = {1908.07782},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1908.07782},
  urldate = {2024-06-12},
  abstract = {The emerging concern about data privacy and security has motivated the proposal of federated learning, which allows nodes to only synchronize the locally-trained models instead their own original data. Conventional federated learning architecture, inherited from the parameter server design, relies on highly centralized topologies and the assumption of large nodes-to-server bandwidths. However, in real-world federated learning scenarios the network capacities between nodes are highly uniformly distributed and smaller than that in a datacenter. It is of great challenges for conventional federated learning approaches to efficiently utilize network capacities between nodes. In this paper, we propose a model segment level decentralized federated learning to tackle this problem. In particular, we propose a segmented gossip approach, which not only makes full utilization of node-to-node bandwidth, but also has good training convergence. The experimental results show that even the training time can be highly reduced as compared to centralized federated learning.},
  pubstate = {preprint},
  keywords = {Computer Science - Distributed Parallel and Cluster Computing,Computer Science - Machine Learning,Computer Science - Networking and Internet Architecture,Statistics - Machine Learning},
  file = {/home/marlon/Zotero/storage/ESAERXAZ/Hu et al. - 2019 - Decentralized Federated Learning A Segmented Goss.pdf;/home/marlon/Zotero/storage/D4DSWWIN/1908.html}
}

@online{kant2024spad,
  title = {{{SPAD}} : {{Spatially Aware Multiview Diffusers}}},
  shorttitle = {{{SPAD}}},
  author = {Kant, Yash and Wu, Ziyi and Vasilkovsky, Michael and Qian, Guocheng and Ren, Jian and Guler, Riza Alp and Ghanem, Bernard and Tulyakov, Sergey and Gilitschenski, Igor and Siarohin, Aliaksandr},
  date = {2024-02-07},
  eprint = {2402.05235},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2402.05235},
  urldate = {2024-04-25},
  abstract = {We present SPAD, a novel approach for creating consistent multi-view images from text prompts or single images. To enable multi-view generation, we repurpose a pretrained 2D diffusion model by extending its self-attention layers with cross-view interactions, and fine-tune it on a high quality subset of Objaverse. We find that a naive extension of the self-attention proposed in prior work (e.g. MVDream) leads to content copying between views. Therefore, we explicitly constrain the cross-view attention based on epipolar geometry. To further enhance 3D consistency, we utilize Plucker coordinates derived from camera rays and inject them as positional encoding. This enables SPAD to reason over spatial proximity in 3D well. In contrast to recent works that can only generate views at fixed azimuth and elevation, SPAD offers full camera control and achieves state-of-the-art results in novel view synthesis on unseen objects from the Objaverse and Google Scanned Objects datasets. Finally, we demonstrate that text-to-3D generation using SPAD prevents the multi-face Janus issue. See more details at our webpage: https://yashkant.github.io/spad},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/marlon/Zotero/storage/FLU39RNU/Kant et al. - 2024 - SPAD  Spatially Aware Multiview Diffusers.pdf;/home/marlon/Zotero/storage/VHC738P8/2402.html}
}

@online{palm2022variational,
  title = {Variational {{Neural Cellular Automata}}},
  author = {Palm, Rasmus Berg and González-Duque, Miguel and Sudhakaran, Shyam and Risi, Sebastian},
  date = {2022-02-02},
  eprint = {2201.12360},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2201.12360},
  urldate = {2024-04-25},
  abstract = {In nature, the process of cellular growth and differentiation has lead to an amazing diversity of organisms -- algae, starfish, giant sequoia, tardigrades, and orcas are all created by the same generative process. Inspired by the incredible diversity of this biological generative process, we propose a generative model, the Variational Neural Cellular Automata (VNCA), which is loosely inspired by the biological processes of cellular growth and differentiation. Unlike previous related works, the VNCA is a proper probabilistic generative model, and we evaluate it according to best practices. We find that the VNCA learns to reconstruct samples well and that despite its relatively few parameters and simple local-only communication, the VNCA can learn to generate a large variety of output from information encoded in a common vector format. While there is a significant gap to the current state-of-the-art in terms of generative modeling performance, we show that the VNCA can learn a purely self-organizing generative process of data. Additionally, we show that the VNCA can learn a distribution of stable attractors that can recover from significant damage.},
  pubstate = {preprint},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  file = {/home/marlon/Zotero/storage/MWVCTXW8/Palm et al. - 2022 - Variational Neural Cellular Automata.pdf;/home/marlon/Zotero/storage/Z5VCQ7LI/2201.html}
}

@online{peebles2023scalable,
  title = {Scalable {{Diffusion Models}} with {{Transformers}}},
  author = {Peebles, William and Xie, Saining},
  date = {2023-03-02},
  eprint = {2212.09748},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2212.09748},
  urldate = {2024-06-04},
  abstract = {We explore a new class of diffusion models based on the transformer architecture. We train latent diffusion models of images, replacing the commonly-used U-Net backbone with a transformer that operates on latent patches. We analyze the scalability of our Diffusion Transformers (DiTs) through the lens of forward pass complexity as measured by Gflops. We find that DiTs with higher Gflops -- through increased transformer depth/width or increased number of input tokens -- consistently have lower FID. In addition to possessing good scalability properties, our largest DiT-XL/2 models outperform all prior diffusion models on the class-conditional ImageNet 512x512 and 256x256 benchmarks, achieving a state-of-the-art FID of 2.27 on the latter.},
  pubstate = {preprint},
  file = {/home/marlon/Zotero/storage/MDICXAL3/Peebles and Xie - 2023 - Scalable Diffusion Models with Transformers.pdf}
}

@online{rombach2022highresolution,
  title = {High-{{Resolution Image Synthesis}} with {{Latent Diffusion Models}}},
  author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Björn},
  date = {2022-04-13},
  eprint = {2112.10752},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2112.10752},
  urldate = {2024-06-04},
  abstract = {By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at https://github.com/CompVis/latent-diffusion .},
  pubstate = {preprint},
  file = {/home/marlon/Zotero/storage/DXDFRYRG/Rombach et al. - 2022 - High-Resolution Image Synthesis with Latent Diffusion Models.pdf}
}

@online{song2023consistency,
  title = {Consistency {{Models}}},
  author = {Song, Yang and Dhariwal, Prafulla and Chen, Mark and Sutskever, Ilya},
  date = {2023-05-31},
  eprint = {2303.01469},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2303.01469},
  urldate = {2024-04-25},
  abstract = {Diffusion models have significantly advanced the fields of image, audio, and video generation, but they depend on an iterative sampling process that causes slow generation. To overcome this limitation, we propose consistency models, a new family of models that generate high quality samples by directly mapping noise to data. They support fast one-step generation by design, while still allowing multistep sampling to trade compute for sample quality. They also support zero-shot data editing, such as image inpainting, colorization, and super-resolution, without requiring explicit training on these tasks. Consistency models can be trained either by distilling pre-trained diffusion models, or as standalone generative models altogether. Through extensive experiments, we demonstrate that they outperform existing distillation techniques for diffusion models in one- and few-step sampling, achieving the new state-of-the-art FID of 3.55 on CIFAR-10 and 6.20 on ImageNet 64x64 for one-step generation. When trained in isolation, consistency models become a new family of generative models that can outperform existing one-step, non-adversarial generative models on standard benchmarks such as CIFAR-10, ImageNet 64x64 and LSUN 256x256.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/marlon/Zotero/storage/2DM3PZGL/Song et al. - 2023 - Consistency Models.pdf;/home/marlon/Zotero/storage/JC64DM27/2303.html}
}

@article{wang2022efficient,
  title = {Efficient {{Ring-Topology Decentralized Federated Learning}} with {{Deep Generative Models}} for {{Medical Data}} in {{eHealthcare Systems}}},
  author = {Wang, Zhao and Hu, Yifan and Yan, Shiyang and Wang, Zhihao and Hou, Ruijie and Wu, Chao},
  date = {2022-05-12},
  journaltitle = {Electronics},
  shortjournal = {Electronics},
  volume = {11},
  number = {10},
  pages = {1548},
  issn = {2079-9292},
  doi = {10.3390/electronics11101548},
  url = {https://www.mdpi.com/2079-9292/11/10/1548},
  urldate = {2024-06-12},
  abstract = {By leveraging deep learning technologies, data-driven-based approaches have reached great success with the rapid increase of data generated for medical applications. However, security and privacy concerns are obstacles for data providers in many sensitive data-driven scenarios, such as rehabilitation and 24 h on-the-go healthcare services. Although many federated learning (FL) approaches have been proposed with DNNs for medical applications, these works still suffer from low usability of data due to data incompleteness, low quality, insufficient quantity, sensitivity, etc. Therefore, we propose a ring-topology-based decentralized federated learning (RDFL) scheme for deep generative models (DGM), where DGM is a promising solution for solving the aforementioned data usability issues. Our RDFL schemes provide communication efficiency and maintain training performance to boost DGMs in target tasks compared with existing FL works. A novel ring FL topology and a map-reduce-based synchronizing method are designed in the proposed RDFL to improve the decentralized FL performance and bandwidth utilization. In addition, an inter-planetary file system (IPFS) is introduced to further improve communication efficiency and FL security. Extensive experiments have been taken to demonstrate the superiority of RDFL with either independent and identically distributed (IID) datasets or non-independent and identically distributed (Non-IID) datasets.},
  langid = {english},
  file = {/home/marlon/Zotero/storage/647N9528/Wang et al. - 2022 - Efficient Ring-Topology Decentralized Federated Le.pdf}
}
